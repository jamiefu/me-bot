{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "message_path = \"./messages.tsv\"\n",
    "\n",
    "DATASET_PATH = \"./message_dataset.pth\"\n",
    "DATALOADER_PATH = \"./message_dataloader.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageDataset(Dataset):\n",
    "    def __init__(self, message_file):\n",
    "        self.filepath = message_file\n",
    "        self.punctuation = {\"?\", \"!\", \":\", \"/\", \";\"}\n",
    "\n",
    "        self.VOCAB_INDEX = {}\n",
    "        self.INDEX_VOCAB = {0: \"<sos>\", 1: \"<eos>\"}\n",
    "        self.word_num = 2\n",
    "\n",
    "        self.prompt = []\n",
    "        self.response = []\n",
    "\n",
    "        self.max_message = 0\n",
    "\n",
    "        with open(self.filepath) as message_file:\n",
    "            reader = list(csv.reader(message_file, delimiter=\"\\t\"))\n",
    "            message_len = len(reader)\n",
    "            for i,row in enumerate(reader):\n",
    "                you = row[0]\n",
    "                me = row[1]\n",
    "\n",
    "                you_tokens = self.tokenize(you)\n",
    "                me_tokens = self.tokenize(me)\n",
    "\n",
    "                for tokens in (you_tokens, me_tokens):\n",
    "                    if len(tokens) > self.max_message:\n",
    "                        self.max_message = len(tokens)\n",
    "                    for token in tokens:\n",
    "                        if token not in self.VOCAB_INDEX:\n",
    "                            self.VOCAB_INDEX[token] = self.word_num\n",
    "                            self.INDEX_VOCAB[self.word_num] = token\n",
    "                            self.word_num += 1\n",
    "                \n",
    "                self.prompt.append(you)\n",
    "                self.response.append(me)\n",
    "                \n",
    "                if i % 1000 == 0:\n",
    "                    print(\"Processed {} out of {} rows in message file\".format(i, message_len))\n",
    "        \n",
    "        self.VOCAB_LEN = len(self.VOCAB_INDEX)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        you = self.tokenize(self.prompt[index])\n",
    "        me = self.tokenize(self.response[index])\n",
    "\n",
    "        you_indices = np.zeros((len(you), 1))\n",
    "        me_indices = np.zeros((len(me), 1))\n",
    "\n",
    "        for i,tok in enumerate(you):\n",
    "            ind = self.VOCAB_INDEX[tok]\n",
    "            you_indices[i] = ind\n",
    "        \n",
    "        for i,tok in enumerate(me):\n",
    "            ind = self.VOCAB_INDEX[tok]\n",
    "            me_indices[i] = ind\n",
    "\n",
    "        you_np = np.asarray(you_indices)\n",
    "        me_np = np.asarray(me_indices)\n",
    "\n",
    "        return torch.LongTensor(you_np), torch.LongTensor(me_np)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompt)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        words = word_tokenize(text)\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Did not load dataset and dataloader from storage\nProcessed 0 out of 21658 rows in message file\nProcessed 1000 out of 21658 rows in message file\nProcessed 2000 out of 21658 rows in message file\nProcessed 3000 out of 21658 rows in message file\nProcessed 4000 out of 21658 rows in message file\nProcessed 5000 out of 21658 rows in message file\nProcessed 6000 out of 21658 rows in message file\nProcessed 7000 out of 21658 rows in message file\nProcessed 8000 out of 21658 rows in message file\nProcessed 9000 out of 21658 rows in message file\nProcessed 10000 out of 21658 rows in message file\nProcessed 11000 out of 21658 rows in message file\nProcessed 12000 out of 21658 rows in message file\nProcessed 13000 out of 21658 rows in message file\nProcessed 14000 out of 21658 rows in message file\nProcessed 15000 out of 21658 rows in message file\nProcessed 16000 out of 21658 rows in message file\nProcessed 17000 out of 21658 rows in message file\nProcessed 18000 out of 21658 rows in message file\nProcessed 19000 out of 21658 rows in message file\nProcessed 20000 out of 21658 rows in message file\nProcessed 21000 out of 21658 rows in message file\nSaved dataset and dataloader at ./message_dataset.pth and ./message_dataloader.pth\n"
    }
   ],
   "source": [
    "try:\n",
    "    dataset = torch.load(DATASET_PATH)\n",
    "    dataloader = torch.load(DATALOADER_PATH)\n",
    "    print(\"Loaded dataset and dataloader from paths\")\n",
    "except:\n",
    "    print(\"Did not load dataset and dataloader from storage\")\n",
    "    dataset = MessageDataset(message_path)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=1, shuffle=True, pin_memory=True, num_workers=2)\n",
    "    # torch.save(dataset, DATASET_PATH)\n",
    "    # torch.save(dataloader, DATALOADER_PATH)\n",
    "    print(\"Saved dataset and dataloader at {} and {}\".format(DATASET_PATH, DATALOADER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " 19117, 'clubbing': 19118, 'rugby': 19119, 'vaping': 19120, 'partaking': 19121, 'psetig': 19122, 'determination': 19123, 'ithis': 19124, 'bolded': 19125, 'measured': 19126, 'imposter': 19127, 'whenre': 19128, 'originaly': 19129, 'cannnn': 19130, 'whichever': 19131, 'gigle': 19132, 'cutee': 19133, 'hellllo': 19134, 'hundred': 19135, 'plagu': 19136, 'aweeeee': 19137, 'whatre': 19138, 'hom': 19139, 'gv': 19140, 'trout': 19141, 'slapped': 19142, 'emalee': 19143, 'liquor': 19144, 'whyre': 19145, 'callls': 19146, 'lychee': 19147, 'alcoholism': 19148, 'tackled': 19149, 'tackle': 19150, 'throgh': 19151, 'jammmmm': 19152, 'thurssaturday': 19153, 'partylong': 19154, 'efuck': 19155, 'frisun': 19156, 'bak': 19157, 'dumbasses': 19158, 'dentists': 19159, 'dinghys': 19160, 'mahbe': 19161, 'listings': 19162, 'pictirws': 19163, 'cuddling': 19164, 'unmarried': 19165, 'miami': 19166, 'weeather': 19167, 'dissapoint': 19168, 'richness': 19169, 'slyly': 19170, 'yacht': 19171, 'estate': 19172, 'volatile': 19173, 'bcg': 19174, 'fame': 19175, 'reassurance': 19176, 'lode': 19177, 'bawled': 19178, 'stupids': 19179, 'beok': 19180, 'baseline': 19181, 'gutted': 19182, 'bitches': 19183, 'zuckerberg': 19184, 'muscular': 19185, 'influencers': 19186, 'diet': 19187, 'togethernot': 19188, 'monthly': 19189, 'purchase': 19190, 'diets': 19191, 'influencer': 19192, 'sustainability': 19193, 'woot': 19194, 'timetraveled': 19195, 'youth': 19196, 'unhappiness': 19197, 'contrast': 19198, 'rewrote': 19199, 'foolish': 19200, 'wink': 19201, 'shrub': 19202, 'wigging': 19203, 'cra': 19204, 'clye': 19205, 'statewide': 19206, 'brendy': 19207, 'bartendinf': 19208, 'dide': 19209, 'hither': 19210, 'hitherfor': 19211, 'littlest': 19212, 'babu': 19213, 'jamu': 19214, 'tii': 19215, 'shriveled': 19216, 'withdrawl': 19217, 'tiktoks': 19218, 'alcoholics': 19219, 'supporting': 19220, 'goooooo': 19221, 'heheheheheheheheh': 19222, 'brooook': 19223, 'earliee': 19224, 'greek': 19225, 'waittt': 19226, 'fakee': 19227, 'frattt': 19228, 'toks': 19229, 'zoomzoom': 19230, 'stricter': 19231, 'relaxers': 19232, 'qualifies': 19233, 'victors': 19234, 'bitcb': 19235, 'paranoif': 19236, 'unfounded': 19237, 'celareful': 19238, 'clorax': 19239, 'wipe': 19240, 'inanimate': 19241, 'cackle': 19242, 'boiled': 19243, 'raeed': 19244, 'whore': 19245, 'opedzoe': 19246, 'youch': 19247, 'scaryyyyy': 19248, 'quaran': 19249, 'tine': 19250, 'listle': 19251, 'listful': 19252, 'forrest': 19253, 'compassionate': 19254, 'zoeeeeeeee': 19255, 'permits': 19256, 'bain': 19257, 'websiteblog': 19258, 'travelers': 19259, 'itineraries': 19260, 'combines': 19261, 'conduct': 19262, 'ect': 19263, 'hehehehehehe': 19264, 'clears': 19265, 'summe': 19266, 'gig': 19267, 'slave': 19268, 'mitten': 19269, 'sprinkle': 19270, 'ehhe': 19271, 'iz': 19272, 'tacked': 19273, 'fizz': 19274, 'amwang': 19275, 'avocado': 19276, 'youuuuuuu': 19277, 'jamfu': 19278, 'cube': 19279, 'progressive': 19280, 'isabelles': 19281, 'refusing': 19282, 'refute': 19283, 'circulate': 19284, 'unasked': 19285, 'revolves': 19286, 'beliefs': 19287, 'gentle': 19288, 'manipulative': 19289, 'punchy': 19290, 'cradley': 19291, 'subverted': 19292, 'rebuttals': 19293, 'educative': 19294, 'translations': 19295, 'rework': 19296, 'tiptoe': 19297, 'crticism': 19298, 'eileens': 19299, 'originap': 19300, 'alink': 19301, 'siding': 19302, 'generalizations': 19303, 'nitpick': 19304, 'disappointment': 19305, 'ungratefulness': 19306, 'adamantly': 19307, 'construing': 19308, 'sayng': 19309, 'adjusted': 19310, 'otberwsie': 19311, 'yeahn': 19312, 'pulse': 19313, 'pacific': 19314, 'crest': 19315, 'trail': 19316, 'solo': 19317, 'vicariously': 19318, 'taker': 19319, 'olivia': 19320, 'rope': 19321, 'brighton': 19322, 'jankiest': 19323, 'cliff': 19324, 'sara': 19325, 'hacked': 19326, 'yayyyyyyy': 19327, 'whennnnn': 19328, 'anytimeeee': 19329, 'yooooo': 19330, 'whatwver': 19331, 'researcher': 19332, 'thid': 19333, 'jshs': 19334, 'forming': 19335, 'fraands': 19336, 'gopf': 19337, 'lions': 19338, 'engineerings': 19339, 'btier': 19340, 'grappling': 19341, 'daayymm': 19342, 'broader': 19343, 'signal': 19344, 'compression': 19345, 'coowner': 19346, 'glancing': 19347, 'unusual': 19348, 'guiding': 19349, 'twll': 19350, 'soulcycle': 19351, 'todaymight': 19352, 'empowering': 19353, 'nsa': 19354, 'hrweek': 19355, 'organizations': 19356, 'gomktzoom': 19357, 'weekens': 19358, 'roo': 19359, 'pineapples': 19360, 'watermelon': 19361, 'jic': 19362, 'anythingunmute': 19363, 'restarting': 19364, 'annaaaaa': 19365, 'aahhh': 19366, 'dayyyy': 19367, 'eyeballing': 19368, 'crave': 19369, 'choco': 19370, 'pearls': 19371, 'tartar': 19372, 'deg': 19373, 'aimlessly': 19374, 'occasional': 19375, 'simmobility': 19376, 'autonomous': 19377, 'demand': 19378, 'pumped': 19379, 'mayybee': 19380, 'afternoons': 19381, 'gohackmitorgapply': 19382, 'muchd': 19383, 'deactivate': 19384, 'busiest': 19385, 'firday': 19386, 'lamyard': 19387, 'helpfull': 19388, 'andyeah': 19389, 'suites': 19390, 'rigid': 19391, 'candidate': 19392, 'adits': 19393, 'externshiip': 19394, 'capped': 19395, 'meetinga': 19396, 'squirrel': 19397, 'shortie': 19398, 'kako': 19399, 'morgan': 19400, 'approximate': 19401, 'seniorcaptain': 19402, 'awards': 19403, 'rearrange': 19404, 'fbs': 19405, 'knwow': 19406, 'wblrubric': 19407, 'tomo': 19408, 'ihni': 19409, 'wwll': 19410, 'packagemodule': 19411, 'angles': 19412, 'chalk': 19413, 'aitht': 19414, 'funcconst': 19415, 'gistgithubcom': 19416, 'totes': 19417, 'everthing': 19418, 'pythoney': 19419, 'pythony': 19420, 'reverse': 19421, 'youore': 19422, 'exerciesse': 19423, 'queens': 19424, 'fundmanetlas': 19425, 'tldr': 19426, 'appply': 19427, 'selfpromoting': 19428, 'asserting': 19429, 'survivng': 19430, 'sucls': 19431, 'mender': 19432, 'aid': 19433, 'audiologist': 19434, 'straighten': 19435, 'fellow': 19436, 'teaming': 19437, 'knkoww': 19438, 'undedicated': 19439, 'baguettes': 19440, 'whaattttt': 19441, 'sl': 19442, 'florez': 19443, 'finboardmitedu': 19444, 'dpatale': 19445, 'agio': 19446, 'treasurer': 19447, 'lerb': 19448, 'initiatives': 19449, 'appointments': 19450, 'outlining': 19451, 'godo': 19452, 'dos': 19453, 'workin': 19454, 'userscore': 19455, 'todos': 19456, 'erics': 19457, 'horizontally': 19458, 'yupucaigmailcom': 19459, 'heyyyyy': 19460, 'shittttttffff': 19461, 'gautam': 19462, 'trapped': 19463, 'sureim': 19464, 'widely': 19465, 'thennn': 19466, 'teyre': 19467, 'pledging': 19468, 'frst': 19469, 'topaying': 19470, 'codirectors': 19471, 'volunteers': 19472, 'deepnnpy': 19473, 'processingpy': 19474, 'ipynb': 19475, 'jamieclstmipynb': 19476, 'accessing': 19477, 'torchtodevice': 19478, 'actuality': 19479, 'ivertime': 19480, 'torchcudaisavailable': 19481, 'disconnected': 19482, 'torif': 19483, 'producing': 19484, 'bceloss': 19485, 'optimizer': 19486, 'adamsgd': 19487, 'missclassifying': 19488, 'tensorfllow': 19489, 'hillarytmitedu': 19490, 'repos': 19491, 'sourcing': 19492, 'tanx': 19493, 'jamessss': 19494, 'likelu': 19495, 'informatics': 19496, 'radiology': 19497, 'aicomputer': 19498, 'imaging': 19499, 'duongs': 19500, 'icic': 19501, 'segmentation': 19502, 'ohmygosh': 19503, 'breast': 19504, 'tensorflows': 19505, 'smtg': 19506, 'yayayy': 19507, 'anywags': 19508, 'substantial': 19509, 'oooohh': 19510, 'noww': 19511, 'interferes': 19512, 'questionss': 19513, 'supposely': 19514, 'benedict': 19515, 'ins': 19516, 'cas': 19517, 'ahahaa': 19518, 'sandals': 19519, 'backstrap': 19520, 'anywhereee': 19521, 'sbu': 19522, 'shoottt': 19523, 'hahahaa': 19524, 'instruments': 19525, 'wiped': 19526, 'urls': 19527, 'setacesscookies': 19528, 'subsequent': 19529, 'getidentity': 19530, 'reenable': 19531, 'integration': 19532, 'quillids': 19533, 'mock': 19534, 'cli': 19535, 'saves': 19536, 'postmans': 19537, 'acess': 19538, 'quillid': 19539, 'rohit': 19540, 'nhacks': 19541, 'stacks': 19542, 'overlords': 19543, 'jenyajpgmailcom': 19544, 'tennisaceing': 19545, 'yeaah': 19546, 'noviaceingautismgmailcom': 19547, 'welovetennis': 19548, 'throuugh': 19549, 'responsibilites': 19550, 'comprehensive': 19551, 'equipment': 19552, 'uk': 19553, 'porque': 19554, 'coupon': 19555, 'jimmyfoodfoundation': 19556, 'cmariecerfgmailcom': 19557, 'departure': 19558, 'ðŸ¥³': 19559, 'spreading': 19560, 'regfox': 19561, 'confiromation': 19562, 'camapaign': 19563, 'addison': 19564, 'flyers': 19565, 'callig': 19566, 'harm': 19567, 'honsetly': 19568, 'brianna': 19569, 'oon': 19570, 'registrations': 19571, 'shailee': 19572, 'mrspatel': 19573, 'lma': 19574, 'nack': 19575, 'knwe': 19576, 'kinras': 19577, 'ville': 19578, 'centria': 19579, 'alic': 19580, 'flyer': 19581, 'sice': 19582, 'preparation': 19583, 'overloading': 19584, 'volleys': 19585, 'supervisor': 19586, 'merit': 19587, 'semifinalist': 19588, 'volunteerng': 19589, 'heah': 19590, 'honestlu': 19591, 'subsidize': 19592, 'academy': 19593, 'lauderdale': 19594, 'jackson': 19595, 'actualhow': 19596, 'glowup': 19597, 'aboutthis': 19598, 'pobably': 19599, 'hulu': 19600, 'showtime': 19601, 'macbooks': 19602, 'loke': 19603, 'vm': 19604, 'boot': 19605, 'xie': 19606, 'praty': 19607, 'commerce': 19608, 'hsit': 19609, 'congratss': 19610, 'grads': 19611, 'yuan': 19612, 'berkllee': 19613, 'coollege': 19614, 'lounging': 19615, 'helllll': 19616, 'kindve': 19617, 'mvs': 19618, 'hellla': 19619, 'posh': 19620, 'ouyang': 19621, 'nana': 19622, 'uuhhh': 19623, 'mayer': 19624, 'titan': 19625, 'naruto': 19626, 'giant': 19627, 'babies': 19628, 'integrating': 19629, 'copycat': 19630, 'palette': 19631, 'likesliked': 19632, 'cleansing': 19633, 'qfs': 19634, 'recycle': 19635, 'poi': 19636, 'iti': 19637, 'ðŸ§¢': 19638, 'advise': 19639, 'brokerage': 19640, 'brokers': 19641, 'facilitating': 19642, 'trades': 19643, 'wins': 19644, 'liquidity': 19645, 'sel': 19646, 'ensuring': 19647, 'pitches': 19648, 'quants': 19649, 'undervalued': 19650, 'reselling': 19651, 'asset': 19652, 'equities': 19653, 'bonds': 19654, 'commodities': 19655, 'exchange': 19656, 'investments': 19657, 'anjlaie': 19658, 'diehard': 19659, 'obsess': 19660, 'religiously': 19661, 'cast': 19662, 'deng': 19663, 'chap': 19664, 'chao': 19665, 'sha': 19666, 'yi': 19667, 'pippin': 19668, 'hinge': 19669, 'nicotine': 19670, 'sights': 19671, 'bracelets': 19672, 'tourists': 19673, 'hooked': 19674, 'sued': 19675, 'bones': 19676, 'bumble': 19677, 'promoting': 19678, 'bumbles': 19679, 'demographic': 19680, 'tangent': 19681, 'banker': 19682, 'bankers': 19683, 'intesne': 19684, 'firms': 19685, 'forarts': 19686, 'outs': 19687, 'mooney': 19688, 'talents': 19689, 'tyle': 19690, 'boringest': 19691, 'sparked': 19692, 'hahhhahahaha': 19693, 'backfired': 19694, 'intiative': 19695, 'frigistick': 19696, 'bragged': 19697, 'iops': 19698, 'exaggerated': 19699, 'sameeeeee': 19700, 'unify': 19701, 'grinded': 19702, 'hirin': 19703, 'hirer': 19704, 'hiree': 19705, 'textin': 19706, 'jfus': 19707, 'kansas': 19708, 'raised': 19709, 'gained': 19710, 'delciiosu': 19711, 'juicy': 19712, 'buns': 19713, 'overlooking': 19714, 'bets': 19715, 'brownstones': 19716, 'abcs': 19717, 'befriend': 19718, 'cbc': 19719, 'dolr': 19720, 'aprtment': 19721, 'vancouver': 19722, 'canaidians': 19723, 'richhh': 19724, 'holyyyyy': 19725, 'predetermined': 19726, 'appearnawa': 19727, 'snarky': 19728, 'outnumber': 19729, 'canadians': 19730, 'dispersing': 19731, 'extinct': 19732, 'replicates': 19733, 'obsessing': 19734, 'renegad': 19735, 'renegading': 19736, 'addictive': 19737, 'hellsite': 19738, 'hardo': 19739, 'poach': 19740, 'rnyu': 19741, 'pyour': 19742, 'samaritan': 19743, 'prefrosh': 19744, 'mitpals': 19745, 'hardos': 19746, 'navigate': 19747, 'iamstern': 19748, 'chu': 19749, 'michel': 19750, 'agreeing': 19751, 'americanized': 19752, 'outerwear': 19753, 'fal': 19754, 'repent': 19755, 'relent': 19756, 'kabout': 19757, 'vastly': 19758, 'udk': 19759, 'stawasz': 19760, 'vividly': 19761, 'ballgame': 19762, 'liberally': 19763, 'appease': 19764, 'replaces': 19765, 'chemical': 19766, 'chemicals': 19767, 'pharma': 19768, 'encompassing': 19769, 'sicence': 19770, 'incest': 19771, 'psychological': 19772, 'depper': 19773, 'pounce': 19774, 'morality': 19775, 'rightness': 19776, 'wrongness': 19777, 'citing': 19778, 'watchdd': 19779, 'tenth': 19780, 'zukos': 19781, 'ponytail': 19782, 'thrill': 19783, 'impatient': 19784, 'crime': 19785, 'thriller': 19786, 'strangers': 19787, 'crimethrillers': 19788, 'pyo': 19789, 'breath': 19790, 'belong': 19791, 'pleb': 19792, 'duality': 19793, 'television': 19794, 'girlfriends': 19795, 'illusion': 19796, 'dated': 19797, 'chivalry': 19798, 'confucian': 19799, 'ohooo': 19800, 'exceeded': 19801, 'beachior': 19802, 'glowed': 19803, 'percentile': 19804, 'nwea': 19805, 'kumin': 19806, 'kumon': 19807, 'proudest': 19808, 'achievement': 19809, 'hahahahaah': 19810, 'multiplication': 19811, 'sizing': 19812, 'woefully': 19813, 'embarassing': 19814, 'mil': 19815, 'hahahahahahha': 19816, 'realised': 19817, 'salvage': 19818, 'krona': 19819, 'believed': 19820, 'externally': 19821, 'facjng': 19822, 'startlans': 19823, 'sophs': 19824, 'prepares': 19825, 'jnderstandable': 19826, 'unmet': 19827, 'stacking': 19828, 'recruited': 19829, 'guesswork': 19830, 'strike': 19831, 'evaluated': 19832, 'trainee': 19833, 'feeder': 19834, 'grunt': 19835, 'portfolios': 19836, 'analysts': 19837, 'managers': 19838, 'strategies': 19839, 'judged': 19840, 'benchmarks': 19841, 'hahahahahahahahahahhaa': 19842, 'routing': 19843, 'volunteeringgg': 19844, 'kaileys': 19845, 'adun': 19846, 'thissssss': 19847, 'crampssss': 19848, 'externingggg': 19849, 'cooool': 19850, 'shoulddd': 19851, 'rotc': 19852, 'postmates': 19853, 'prqctice': 19854, 'intrigued': 19855, 'tuscaninis': 19856, 'prwctjc': 19857, 'alteafy': 19858, 'practiceee': 19859, 'disney': 19860, 'jupneeet': 19861, 'friendsd': 19862, 'scrimmage': 19863, 'spon': 19864, 'decoded': 19865, 'ilyily': 19866, 'oneonones': 19867, 'katherinexliugmailcom': 19868, 'tns': 19869, 'wlillian': 19870, 'yeahhhhhh': 19871, 'impress': 19872, 'koreanness': 19873, 'gently': 19874, 'awesomely': 19875, 'lowe': 19876, 'endoftheyear': 19877, 'shoutout': 19878, 'besant': 19879, 'joshi': 19880, 'commonwealth': 19881, 'intends': 19882, 'xu': 19883, 'varsity': 19884, 'roasts': 19885, 'importantly': 19886, 'greenhills': 19887, 'unknowingly': 19888, 'competitively': 19889, 'perseverance': 19890, 'clift': 19891, 'hnghhhhh': 19892, 'peaked': 19893, 'seeded': 19894, 'thoooo': 19895, 'tia': 19896, 'denali': 19897, 'recreational': 19898, 'finishes': 19899, 'restring': 19900, 'looser': 19901, 'restringing': 19902, 'gots': 19903, 'expcetion': 19904, 'businesss': 19905, 'booming': 19906, 'offense': 19907, 'jordana': 19908, 'backhand': 19909, 'forehand': 19910, 'tammy': 19911, 'topspin': 19912, 'andelu': 19913, 'leggo': 19914, 'lift': 19915, 'ugrh': 19916, 'sksksk': 19917, 'charli': 19918, 'xcx': 19919, 'boppy': 19920, 'debut': 19921, 'facetimes': 19922, 'fans': 19923, 'abix': 19924, 'mxm': 19925, 'ygjyp': 19926, 'topped': 19927, 'seongwu': 19928, 'friendzone': 19929, 'zoning': 19930, 'askin': 19931, 'pulll': 19932, 'abg': 19933, 'goddamnn': 19934, 'falsies': 19935, 'plaaaay': 19936, 'priv': 19937, 'malec': 19938, 'intents': 19939, 'mcihelle': 19940, 'disgustingly': 19941, 'honeymoon': 19942, 'thrivin': 19943, 'ehf': 19944, 'dye': 19945, 'blonde': 19946, 'pale': 19947, 'pastel': 19948, 'pinks': 19949, 'haired': 19950, 'wewh': 19951, 'tattoos': 19952, 'yeaaah': 19953, 'retouching': 19954, 'vestiges': 19955, 'shoulders': 19956, 'chopping': 19957, 'chop': 19958, 'dieting': 19959, 'bald': 19960, 'eassyyyyy': 19961, 'ples': 19962, 'leaveeee': 19963, 'ofcofc': 19964, 'katieeee': 19965, 'hamieeee': 19966, 'yusss': 19967, 'hiyaaa': 19968, 'addressed': 19969, 'condemning': 19970, 'deter': 19971, 'condemned': 19972, 'ignorant': 19973, 'maithelee': 19974, 'acknowledging': 19975, 'thoughtsreflect': 19976, 'unpublished': 19977, 'selfisolation': 19978, 'novelistic': 19979, 'asdfghjkl': 19980, 'honors': 19981, 'langley': 19982, 'velocity': 19983, 'cello': 19984, 'regrets': 19985, 'montue': 19986, 'lthe': 19987, 'krittamate': 19988, 'oww': 19989, 'himher': 19990, 'maseehtype': 19991, 'latecomers': 19992, 'wwtch': 19993, 'whao': 19994, 'hassles': 19995, 'grumbles': 19996, 'dumbaas': 19997, 'mhhmmm': 19998, 'sundah': 19999, 'pdi': 20000, 'fude': 20001, 'stufy': 20002, 'ingernship': 20003, 'pshhhh': 20004, 'jes': 20005, 'mistook': 20006, 'textbooks': 20007, 'omfggg': 20008, 'theorems': 20009, 'refresher': 20010, 'corrections': 20011, 'magnetic': 20012, 'eyelashes': 20013, 'bills': 20014, 'michellee': 20015, 'refers': 20016, 'dq': 20017, 'fanout': 20018, 'nbit': 20019, 'muxes': 20020, 'buffersinverters': 20021, 'gates': 20022, 'chooses': 20023, 'buffers': 20024, 'propagation': 20025, 'delays': 20026, 'approximately': 20027, 'inv': 20028, 'buf': 20029, 'gate': 20030, 'drives': 20031, 'amplify': 20032, 'currentdriving': 20033, 'miless': 20034, 'dqs': 20035, 'redirected': 20036, 'mitzoomusmymilesdai': 20037, 'concurrency': 20038, 'indexed': 20039, 'elena': 20040, 'boal': 20041, 'pipelining': 20042, 'veronica': 20043, 'muriga': 20044, 'propagating': 20045, 'royally': 20046, 'rechecked': 20047, 'beveren': 20048, 'makedirectmappedbeveren': 20049, 'produces': 20050, 'reqenabled': 20051, 'bool': 20052, 'stateready': 20053, 'takeher': 20054, 'lookup': 20055, 'ifstatements': 20056, 'conditionals': 20057, 'printout': 20058, 'iamroot': 20059, 'aliases': 20060, 'iamrootmitedu': 20061, 'masterful': 20062, 'soylent': 20063, 'lmso': 20064, 'blasphemous': 20065, 'pont': 20066, 'ker': 20067, 'kern': 20068, 'mqliu': 20069, 'kaspers': 20070, 'yiheng': 20071, 'acapella': 20072, 'callbacks': 20073, 'biaoge': 20074, 'biaomeii': 20075, 'kersey': 20076, 'babyyy': 20077, 'sometimeeee': 20078, 'michigans': 20079, 'recording': 20080, 'ze': 20081, 'determining': 20082, 'wiling': 20083, 'binding': 20084, 'fok': 20085, 'artistic': 20086, 'hwite': 20087, 'stuy': 20088, 'dreshmen': 20089, 'caitlin': 20090, 'cheapass': 20091, 'heloooo': 20092, 'opheliaaaaaa': 20093, 'xddd': 20094, 'reluctant': 20095, 'eraser': 20096, 'capitalism': 20097, 'tytyy': 20098, 'omgggggg': 20099, 'almsot': 20100, 'jiffy': 20101, 'embarrass': 20102, 'adp': 20103, 'nontechnical': 20104, 'competing': 20105, 'therell': 20106, 'heya': 20107, 'certified': 20108, 'poofy': 20109, 'hershey': 20110, 'allergoc': 20111, 'tk': 20112, 'vcstartup': 20113, 'entrepralooza': 20114, 'avichal': 20115, 'assets': 20116, 'wheneverr': 20117, 'knightseangmailcom': 20118, 'esle': 20119, 'somoe': 20120, 'linden': 20121, 'llindenmitedu': 20122, 'jiannamitedu': 20123, 'ckdongmitedu': 20124, 'akinimitedu': 20125, 'liukatmitedu': 20126, 'zou': 20127, 'ezoumitedu': 20128, 'zachary': 20129, 'villaverde': 20130, 'zacharyvmitedu': 20131, 'drafted': 20132, 'sids': 20133, 'dopee': 20134, 'konwoos': 20135, 'garciasimons': 20136, 'selloyt': 20137, 'defaulted': 20138, 'yeye': 20139, 'shiit': 20140, 'lecturerecitation': 20141, 'tangential': 20142, 'proofsolving': 20143, 'biimplication': 20144, 'reduces': 20145, 'underwhelming': 20146, 'tanked': 20147, 'friggin': 20148, 'respectable': 20149, 'rediscover': 20150, 'ted': 20151, 'yellowstone': 20152, 'ooops': 20153, 'makegeneral': 20154, 'xfairdevand': 20155, 'presumptuous': 20156, 'tiffanyy': 20157, 'rambling': 20158, 'deets': 20159, 'integrated': 20160, 'studentrun': 20161, 'aims': 20162, 'empower': 20163, 'cuttingedge': 20164, 'techinnovators': 20165, 'talented': 20166, 'showcasing': 20167, 'demos': 20168, 'makeathon': 20169, 'hundreds': 20170, 'makers': 20171, 'muchhh': 20172, 'origianl': 20173, 'onbooarding': 20174, 'saodeskmitedu': 20175, 'studd': 20176, 'onboardingknowledge': 20177, 'subscribed': 20178, 'yappie': 20179, 'wongfu': 20180, 'dyinig': 20181, 'bostn': 20182, 'meself': 20183, 'michal': 20184, 'seri': 20185, 'suspended': 20186, 'suspension': 20187, 'floods': 20188, 'outbreak': 20189, 'nation': 20190, 'ðŸ¥ºðŸ¥ºðŸ¥º': 20191, 'barbershops': 20192, 'bushcamper': 20193, 'shack': 20194, 'immense': 20195, 'aah': 20196, 'snacco': 20197, 'dayyyyyy': 20198, 'wells': 20199, 'kamindowmsuedu': 20200, 'settingkey': 20201}\n"
    }
   ],
   "source": [
    "print(dataset.VOCAB_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_tensor = torch.transpose(input_tensor, 0, 1).squeeze(1).squeeze(1)\n",
    "    target_tensor = torch.transpose(target_tensor, 0, 1).squeeze(1).squeeze(1)\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    if loss != 0:\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        return loss.item() / target_length\n",
    "    else:\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "801\n"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "# NEED TO SET\n",
    "vocab_words = dataset.VOCAB_LEN\n",
    "max_length = dataset.max_message\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, dataloader, epochs, temp_path, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    prev_loss_avg = None\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        for prompt,response in dataloader:\n",
    "            input_tensor = prompt\n",
    "            target_tensor = response\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                        decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "        else:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d) %.4f' % (timeSince(start, i / epochs), print_loss_avg))\n",
    "            if prev_loss_avg is None or print_loss_avg < prev_loss_avg:\n",
    "                torch.save({encoder: encoder.state_dict(), decoder: decoder.state_dict()}, temp_path)\n",
    "                print(\"Saved model at temp path!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'item'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-184d3f89eb83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfinal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./bot.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-f3c21d865b43>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, dataloader, epochs, temp_path, learning_rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 18\u001b[0;31m                         decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-dc403cdb4653>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(vocab_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, vocab_words, max_length, dropout_p=0.1).to(device)\n",
    "\n",
    "epochs = 2\n",
    "temp_path = \"./bot_temp.pth\"\n",
    "final_path = \"./bot.pth\"\n",
    "\n",
    "trainIters(encoder, decoder, dataloader, epochs, max_length)\n",
    "torch.save({encoder: encoder.state_dict(), decoder: decoder.state_dict()}, final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, dataset, text, max_length):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = build_input(text, dataset)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dataset.INDEX_VOCAB[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input(text, dataset):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    indices = np.zeros((len(you), 1))\n",
    "    for i,tok in enumerate(words):\n",
    "        ind = dataset.VOCAB_INDEX[tok]\n",
    "        indices[i] = ind\n",
    "    ind_np = np.asarray(indices)\n",
    "\n",
    "    return torch.LongTensor(ind_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(text, encoder=encoder, decoder=decoder, dataset=dataset):\n",
    "    output_words, attentions = evaluate(encoder, decoder, datset, text, max_length)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print(output_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('bot': conda)",
   "language": "python",
   "name": "python37764bitbotconda9369819fefc84a6c9e2b37b1770a4808"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}